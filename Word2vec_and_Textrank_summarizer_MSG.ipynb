{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mohammad Ghadban - 20.05-2019\n",
    "'''\n",
    "    First step here is to download and import the nessesary \n",
    "    libraries such as follows:\n",
    "    \n",
    "    1. importing gzip and zipfile for the compressing and decompressing\n",
    "    of the downloaded files.\n",
    "    \n",
    "    2. importing gensim for unsupervised semantic \n",
    "    modelling from plain text.\n",
    "    \n",
    "    3. importin the logging to watch the flow in the development\n",
    "    process.\n",
    "    \n",
    "'''\n",
    "#!pip3 install -qU gensim==3.6.0\n",
    "import gzip #compress and decompress\n",
    "from zipfile import ZipFile #compress and decompress\n",
    "import gensim #unsupervised semantic modelling from plain text\n",
    "import logging #showing the flow of development \n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the zip file name \n",
    "file_name = \"data_downloaded/word-embeddings.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode \n",
    "with ZipFile(file_name, 'r') as zip: \n",
    "    # printing all the contents of the zip file \n",
    "    zip.printdir() \n",
    "  \n",
    "    # extracting all the files \n",
    "    print('Extracting all the files now...') \n",
    "    zip.extractall() \n",
    "    print('Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with open('glove_emb.pkl', 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Understanding some of the parameters:\n",
    "    To train the model, we have to set some parameters. \n",
    "    Now, let's try to understand what some of them mean. \n",
    "    For reference, this is the command that we used to train the model.\n",
    "    \n",
    "    Size:\n",
    "    the dense vector to represent each toke or word.\n",
    "    If you have limited data then the size should be a much smaller value.\n",
    "    If you have lots of data, its good to experiment with various sizes.\n",
    "    A value of 100 - 150 might work well, but you need to experiment.\n",
    "    \n",
    "    Window:\n",
    "    The maximum distance between \n",
    "    the target word and its neighboring word.\n",
    "    If your neighbor's position\n",
    "    is greater than the maximum window width to the left and the right, \n",
    "    then, some neighbors are not considered\n",
    "    as being related to the target word. \n",
    "    In theory, a smaller window \n",
    "    should give you terms that are more related. \n",
    "    If you have lots of data,\n",
    "    then the window size should not matter too much, \n",
    "    as long as its a decent sized window\n",
    "    \n",
    "    \n",
    "    min_count:\n",
    "    Minimium frequency count of words.\n",
    "    The model would ignore words that do not statisfy the min_count.\n",
    "    Extremely infrequent words are usually unimportant,\n",
    "    so its best to get rid of those. \n",
    "    Unless your dataset is really tiny,\n",
    "    this does not really affect the model.\n",
    "\n",
    "    \n",
    "    Workers:\n",
    "    How many threads to use behind the scenes?\n",
    "    \n",
    "'''\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are saving the model in i abinary file and a txt file\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, KeyedVectors   \n",
    "model.wv.save_word2vec_format('model.bin', binary=True)\n",
    "model.wv.save_word2vec_format('model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(\n",
    "    ('model.txt'), \n",
    "    binary=False)  # C text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now lets look at some outout:\n",
    "    This first example shows a simple case \n",
    "    of looking up words similar to the word dirty. \n",
    "    All we need to do here is to call the most_similar function\n",
    "    and provide the word dirty as the positive example. \n",
    "    This returns the top 10 similar words.\n",
    "\n",
    "\n",
    "    Example\n",
    "    ---------------------------\n",
    "''' \n",
    "\n",
    "w1 = 'dirty'\n",
    "wv_from_text.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"car\"]\n",
    "wv_from_text.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"politics\"]\n",
    "wv_from_text.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"weather\",'sun','blue']\n",
    "w2 = ['cold']\n",
    "wv_from_text.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "wv_from_text.similarity(w1=\"rain\",w2=\"cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "wv_from_text.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the article \n",
    "file = open('data_downloaded/nature_moon.txt', 'r').read()\n",
    "file =   \"\"+ file + \"\"\n",
    "file = file.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a table with columns article_text and article_id\n",
    "and showing the first 5 rows \n",
    "'''\n",
    "df = pd.DataFrame(file, columns=['article_text'])\n",
    "df['article_id'] = range(len(df['article_text']))\n",
    "print('The first five rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text in the articles into sentences\n",
    "\n",
    "#creating an empty list \n",
    "sentences = []\n",
    "\n",
    "for s in df['article_text']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "\n",
    "# flatten the list\n",
    "sentences = [y for x in sentences for y in x]\n",
    "\n",
    "#Showing the first 10 sentences \n",
    "#sentences[1:10]\n",
    "df.head()\n",
    "print(sentences[1:5])\n",
    "\n",
    "#showing the length of the list og sentences\n",
    "print('\\n\\nlength of sentences: ',len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "print('CLean text:\\n')\n",
    "clean_sentences[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')# one time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the english stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "print(clean_sentences[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('model.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((150,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((150,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,150), sentence_vectors[j].reshape(1,150))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of sentences to form the summary\n",
    "sn = 1\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of sentences to form the summary\n",
    "sn = 5\n",
    "textdone = []\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    textdone.append(ranked_sentences[i][1])\n",
    "\n",
    "print('Summary:')\n",
    "for t in textdone:\n",
    "    print((t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary.\n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we could chose other approaches regargind the initializing word-embeddings such as follows:\n",
    "- Choosing other word2vec or word embediddng data sets\n",
    "- Making own word2vec/ word-embedding data sets\n",
    "- The file choosen for this code in my openion is way too big for such asimple task.\n",
    "    \n",
    "Have a good one :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
