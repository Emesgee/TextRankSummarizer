# Read Me

#### If you want to see the results please choose the 'Word2vec_and_Textrank_summarizer_MSG.html'
GO HERE http://htmlpreview.github.io/
Add the link https://github.com/Emesgee/TextRankSummarizer/blob/master/Word2vec_and_Textrank_summarizer_MSG.html

### Nature_moon.txt
Link for the article "The Moonâ€™s mantle unveiled" can be found here: https://www.nature.com/articles/d41586-019-01479-x

I've used pretrained word-embeddings from 
 


### word-embeddings I've used is from Kaggle
2 million word vectors trained on Common Crawl (600B tokens)
300-dimensional pretrained FastText English word vectors released by Facebook.
The first line of the file contains the number of words in the vocabulary and the size of the vectors. 
Each line contains a word followed by its vectors, like in the default fastText text format.
Each value is space separated. Words are ordered by descending frequency.

The link: https://www.kaggle.com/yimacs/word-embeddings/downloads/word-embeddings.zip/1

Have in mind that the word-embedding data is way to large and u can easily find data with aroung 500mb of data, what is good enough
to make a descent text summarizer.

You can also make your own word-embeddings, but that will wait for another repository.

### Enoy and great coding...


Mohammad Ghadban
